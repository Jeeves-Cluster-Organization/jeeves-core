# ==============================================================================
# Jeeves FF - Environment Configuration
# ==============================================================================
#
# Usage:
#   docker compose up -d
#
# Per Engineering Plan v4.2: Single source of truth for configuration.
# These values are also used by docker-compose.yml via ${VAR:-default} syntax.
#
# ==============================================================================

# ------------------------------------------------------------------------------
# DATABASE (PostgreSQL + pgvector)
# ------------------------------------------------------------------------------
DATABASE_BACKEND=postgres
VECTOR_BACKEND=pgvector

# PostgreSQL connection (matches docker-compose.yml defaults)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DATABASE=assistant
POSTGRES_USER=assistant
POSTGRES_PASSWORD=dev_password_change_in_production

# ------------------------------------------------------------------------------
# LLM PROVIDER (llama-server)
# ------------------------------------------------------------------------------
LLM_PROVIDER=llamaserver
LLAMASERVER_HOST=http://localhost:8080
DEFAULT_MODEL=qwen2.5-3b-instruct

# llama-server configuration (used by docker-compose)
# Default: Qwen 2.5-3B (2GB, good balance of speed and quality)
# Alternative: Qwen2.5-7B-Instruct-Q4_K_M.gguf (4.68GB, better quality)
LLAMA_MODEL=qwen2.5-3b-instruct-q4_k_m.gguf
# CRITICAL: Context window must be >= 16384 for Qwen2.5-3B
# Agents use num_ctx=16384 (see agents/code_analysis/*.py).
#
# IMPORTANT: With --parallel N, context is divided across N slots:
#   n_ctx_per_slot = LLAMA_CTX_SIZE / LLAMA_PARALLEL
# So with 16384 / 4 = 4096 per slot, prompts may get truncated!
# Setting LLAMA_PARALLEL=1 ensures full context per request.
LLAMA_CTX_SIZE=16384
LLAMA_GPU_LAYERS=35
LLAMA_PARALLEL=1

# Per-agent model overrides (optional - uses DEFAULT_MODEL if not set)
# Note: llama-server loads a single model, so these are informational only
# PLANNER_MODEL=qwen2.5-7b-instruct-q4_K_M
# VALIDATOR_MODEL=qwen2.5-7b-instruct-q4_K_M
# CRITIC_MODEL=qwen2.5-7b-instruct-q4_K_M
# META_VALIDATOR_MODEL=qwen2.5-7b-instruct-q4_K_M

# Temperature settings (optional - uses config/constants.py defaults if not set)
# PLANNER_TEMPERATURE=0.3
# VALIDATOR_TEMPERATURE=0.3
# CRITIC_TEMPERATURE=0.2
# META_VALIDATOR_TEMPERATURE=0.7

# Cloud providers (only if LLM_PROVIDER != llamaserver)
# OPENAI_API_KEY=sk-...
# ANTHROPIC_API_KEY=sk-ant-...
# AZURE_ENDPOINT=https://...
# AZURE_API_KEY=...

# ------------------------------------------------------------------------------
# API SERVER
# ------------------------------------------------------------------------------
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=DEBUG

# ------------------------------------------------------------------------------
# FEATURES
# ------------------------------------------------------------------------------
MEMORY_ENABLED=true
META_VALIDATION_ENABLED=true

# ------------------------------------------------------------------------------
