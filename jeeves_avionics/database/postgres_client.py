"""Async PostgreSQL client with pgvector support using SQLAlchemy."""

import asyncio
import json
from contextlib import asynccontextmanager
from datetime import datetime
from typing import Any, Dict, List, Optional, AsyncIterator
from pathlib import Path
import re
from itertools import count
from uuid import UUID
from sqlalchemy.ext.asyncio import (
    create_async_engine,
    AsyncSession,
    AsyncEngine,
    async_sessionmaker,
)
from sqlalchemy import text
from sqlalchemy.pool import NullPool

from jeeves_avionics.logging import get_current_logger
from jeeves_shared.serialization import JSONEncoderWithUUID, to_json, from_json
from jeeves_avionics.database.constants import UUID_COLUMNS, JSONB_COLUMNS, VECTOR_COLUMNS
from jeeves_protocols import LoggerProtocol

# Regex pattern for UUID strings (8-4-4-4-12 hex format)
UUID_PATTERN = re.compile(
    r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$',
    re.IGNORECASE
)


class PostgreSQLClient:
    """Async PostgreSQL client with helper methods and pgvector support."""

    def __init__(
        self,
        database_url: str,
        pool_size: int = 20,
        max_overflow: int = 10,
        pool_timeout: int = 30,
        pool_recycle: int = 3600,
        echo: bool = False,
        logger: Optional[LoggerProtocol] = None,
    ):
        """Initialize PostgreSQL client.

        Args:
            database_url: PostgreSQL connection URL (postgresql+asyncpg://...)
            pool_size: Maximum number of connections in the pool
            max_overflow: Maximum overflow connections beyond pool_size
            pool_timeout: Timeout for getting a connection from the pool
            pool_recycle: Recycle connections after this many seconds
            echo: Echo SQL statements for debugging
            logger: Logger for DI (uses context logger if not provided)
        """
        self._logger = logger or get_current_logger()
        self.database_url = database_url
        self.pool_size = pool_size
        self.max_overflow = max_overflow
        self.pool_timeout = pool_timeout
        self.pool_recycle = pool_recycle
        self.echo = echo
        self.backend = "postgres"  # Backend identifier

        self.engine: Optional[AsyncEngine] = None
        self.session_factory: Optional[async_sessionmaker] = None
        self._initialized = False

    @property
    def pool(self):
        """Expose engine's connection pool for compatibility."""
        return self.engine.pool if self.engine else None

    @staticmethod
    def _convert_uuid_strings(value: Any, param_name: str = None) -> Any:
        """Minimal UUID conversion for PostgreSQL compatibility.

        Only converts UUID objects to strings when needed. Does NOT auto-convert
        strings to avoid circular conversion issues.

        Args:
            value: The value to potentially convert
            param_name: Optional parameter name (unused, for API compatibility)

        Returns:
            Converted value
        """
        # Pass through all values unchanged
        # UUID strings work fine with asyncpg - it handles conversion automatically
        # Tests should use proper UUIDs (use test_uuid() helper in conftest.py)
        return value

    @staticmethod
    def _prepare_query_and_params(
        query: str,
        parameters: Optional[Any]
    ) -> (str, Dict[str, Any]):
        """Convert SQLite-style ? params to named parameters for SQLAlchemy.

        Supports:
        - None (no params)
        - dict (pass-through with UUID conversion)
        - tuple/list (replaces each ? with :p0/:p1/etc, with UUID conversion)

        UUID-formatted strings are automatically converted to uuid.UUID objects
        for asyncpg compatibility (PostgreSQL UUID columns require UUID objects).
        """
        if parameters is None:
            return query, {}

        if isinstance(parameters, dict):
            # Import uuid_str for UUID handling at ingestion layer
            from jeeves_shared.uuid_utils import uuid_str

            # Convert UUID values for known UUID columns (from centralized constants)
            converted = {}
            for k, v in parameters.items():
                if k in UUID_COLUMNS and v is not None:
                    converted[k] = uuid_str(v)
                else:
                    converted[k] = v

            return query, converted

        if isinstance(parameters, (list, tuple)):
            # Import uuid_str for UUID handling at ingestion layer
            from jeeves_shared.uuid_utils import uuid_str

            # Extract column names from query (UUID_COLUMNS imported from centralized constants)
            column_matches = []

            # First try INSERT: INSERT INTO table (col1, col2, ...) VALUES (?, ?, ...)
            insert_match = re.search(
                r'INSERT\s+INTO\s+\w+\s*\(([^)]+)\)\s*VALUES',
                query,
                re.IGNORECASE
            )
            if insert_match:
                # Parse column names from INSERT column list
                col_list = insert_match.group(1)
                column_matches = [c.strip() for c in col_list.split(',')]
            else:
                # Fall back to WHERE clause patterns (col = ?)
                column_matches = re.findall(r'(\w+)\s*=\s*\?', query, re.IGNORECASE)

            param_iter = iter(parameters)
            counter = count()

            def repl(match: re.Match) -> str:
                idx = next(counter)
                key = f"p{idx}"
                value = next(param_iter)
                return f":{key}"

            # Replace each ? with a named parameter placeholder
            new_query = re.sub(r"\?", repl, query)

            # Build parameter dict with UUID conversion for known UUID columns
            params = {}
            for idx, val in enumerate(parameters):
                col_name = column_matches[idx] if idx < len(column_matches) else None
                if col_name in UUID_COLUMNS and val is not None:
                    params[f"p{idx}"] = uuid_str(val)
                else:
                    params[f"p{idx}"] = val

            return new_query, params

        raise TypeError(f"Unsupported parameter type: {type(parameters)}")

    async def connect(self):
        """Establish database connection and create engine."""
        if self._initialized:
            self._logger.warning("postgres_already_connected")
            return

        try:
            # Create async engine with connection pooling
            # Note: For async engines, SQLAlchemy automatically uses AsyncAdaptedQueuePool
            # We don't specify poolclass explicitly - it handles async pooling correctly
            self.engine = create_async_engine(
                self.database_url,
                pool_size=self.pool_size,
                max_overflow=self.max_overflow,
                pool_timeout=self.pool_timeout,
                pool_recycle=self.pool_recycle,
                echo=self.echo,
                # Connection arguments
                connect_args={
                    "server_settings": {
                        "application_name": "7-agent-assistant",
                        "jit": "off",  # Disable JIT for better connection startup time
                    }
                },
            )

            # Create session factory
            self.session_factory = async_sessionmaker(
                self.engine,
                class_=AsyncSession,
                expire_on_commit=False,
            )

            # Test connection and verify pgvector extension
            async with self.engine.begin() as conn:
                # Check connection
                result = await conn.execute(text("SELECT version()"))
                version = result.scalar()
                self._logger.info("postgres_connected", version=version[:50])

                # Verify pgvector extension
                result = await conn.execute(
                    text("SELECT extname, extversion FROM pg_extension WHERE extname = 'vector'")
                )
                pgvector_info = result.fetchone()
                if pgvector_info:
                    self._logger.info(
                        "pgvector_extension_verified",
                        version=pgvector_info[1]
                    )
                else:
                    self._logger.warning("pgvector_extension_not_found")

            self._initialized = True

        except Exception as e:
            self._logger.error("postgres_connection_failed", error=str(e))
            raise

    async def disconnect(self):
        """Close database connection and dispose engine."""
        if not self._initialized:
            return

        try:
            if self.engine:
                await self.engine.dispose()
                self._logger.info("postgres_disconnected")
        except Exception as e:
            self._logger.error("postgres_disconnect_failed", error=str(e))
        finally:
            self._initialized = False
            self.engine = None
            self.session_factory = None

    async def close(self):
        """Alias for disconnect() for consistency with other APIs."""
        await self.disconnect()

    @asynccontextmanager
    async def session(self) -> AsyncIterator[AsyncSession]:
        """Get a database session context manager.

        Usage:
            async with client.session() as session:
                result = await session.execute(query)
                await session.commit()
        """
        if not self._initialized:
            raise RuntimeError("Database not connected. Call connect() first.")

        if not self.session_factory:
            raise RuntimeError("Session factory not initialized")

        async with self.session_factory() as session:
            yield session

    @asynccontextmanager
    async def transaction(self) -> AsyncIterator[AsyncSession]:
        """Get a database transaction context manager.

        Automatically commits on success, rolls back on error.

        Usage:
            async with client.transaction() as session:
                await session.execute(query1)
                await session.execute(query2)
                # Automatic commit on exit, rollback on exception
        """
        async with self.session() as session:
            try:
                async with session.begin():
                    yield session
            except Exception:
                # Ensure rollback on any exception
                # session.begin() should handle this, but be explicit for safety
                if session.in_transaction():
                    await session.rollback()
                raise

    async def initialize_schema(self, schema_path: str = "database/schemas/postgres_schema.sql"):
        """Initialize database schema from SQL file.

        Args:
            schema_path: Path to the SQL schema file
        """
        schema_file = Path(schema_path)
        if not schema_file.exists():
            raise FileNotFoundError(f"Schema file not found: {schema_path}")

        schema_sql = schema_file.read_text()

        def _split_sql_statements(sql: str) -> List[str]:
            """Split SQL into executable statements while honoring dollar quoting and comments."""
            statements: List[str] = []
            current: List[str] = []
            in_single = False
            in_double = False
            dollar_delim: Optional[str] = None
            in_line_comment = False
            in_block_comment = False
            i = 0

            def startswith_dollar_delim(text: str, pos: int) -> Optional[str]:
                match = re.match(r"\$[A-Za-z0-9_]*\$", text[pos:])
                return match.group(0) if match else None

            while i < len(sql):
                ch = sql[i]
                nxt = sql[i + 1] if i + 1 < len(sql) else ""

                if in_line_comment:
                    current.append(ch)
                    if ch == "\n":
                        in_line_comment = False
                    i += 1
                    continue

                if in_block_comment:
                    current.append(ch)
                    if ch == "*" and nxt == "/":
                        current.append(nxt)
                        in_block_comment = False
                        i += 2
                    else:
                        i += 1
                    continue

                if dollar_delim:
                    if sql.startswith(dollar_delim, i):
                        current.append(dollar_delim)
                        i += len(dollar_delim)
                        dollar_delim = None
                    else:
                        current.append(ch)
                        i += 1
                    continue

                if in_single:
                    current.append(ch)
                    if ch == "'" and nxt == "'":
                        current.append(nxt)
                        i += 2
                    elif ch == "'":
                        in_single = False
                        i += 1
                    else:
                        i += 1
                    continue

                if in_double:
                    current.append(ch)
                    if ch == '"' and nxt == '"':
                        current.append(nxt)
                        i += 2
                    elif ch == '"':
                        in_double = False
                        i += 1
                    else:
                        i += 1
                    continue

                if ch == "-" and nxt == "-":
                    current.extend([ch, nxt])
                    in_line_comment = True
                    i += 2
                    continue

                if ch == "/" and nxt == "*":
                    current.extend([ch, nxt])
                    in_block_comment = True
                    i += 2
                    continue

                if ch == "'":
                    in_single = True
                    current.append(ch)
                    i += 1
                    continue

                if ch == '"':
                    in_double = True
                    current.append(ch)
                    i += 1
                    continue

                potential_dollar = startswith_dollar_delim(sql, i)
                if potential_dollar:
                    dollar_delim = potential_dollar
                    current.append(potential_dollar)
                    i += len(potential_dollar)
                    continue

                if ch == ";":
                    statement = "".join(current).strip()
                    if statement:
                        statements.append(statement)
                    current = []
                    i += 1
                    continue

                current.append(ch)
                i += 1

            tail = "".join(current).strip()
            if tail:
                statements.append(tail)

            return statements

        statements = _split_sql_statements(schema_sql)

        async with self.engine.begin() as conn:
            for statement in statements:
                try:
                    await conn.execute(text(statement))
                except Exception as e:
                    self._logger.error(
                        "schema_statement_failed",
                        statement=statement[:100],
                        error=str(e)
                    )
                    raise

        self._logger.info("postgres_schema_initialized", statements=len(statements))

    async def execute(
        self,
        query: str,
        parameters: Optional[Dict[str, Any]] = None
    ) -> Any:
        """Execute a query with parameters.

        Args:
            query: SQL query string (can use :param style parameters)
            parameters: Dictionary of parameters

        Returns:
            Result proxy from SQLAlchemy
        """
        if not self._initialized:
            raise RuntimeError("Database not connected")

        prepared_query, prepared_params = self._prepare_query_and_params(query, parameters)

        async with self.session() as session:
            result = await session.execute(text(prepared_query), prepared_params)
            await session.commit()
            return result

    async def execute_many(
        self,
        query: str,
        parameters_list: List[Dict[str, Any]]
    ) -> None:
        """Execute a query multiple times with different parameters.

        Args:
            query: SQL query string
            parameters_list: List of parameter dictionaries
        """
        if not self._initialized:
            raise RuntimeError("Database not connected")

        async with self.transaction() as session:
            for parameters in parameters_list:
                prepared_query, prepared_params = self._prepare_query_and_params(
                    query,
                    parameters
                )
                await session.execute(text(prepared_query), prepared_params)

    async def execute_script(self, script: str) -> None:
        """Execute a multi-statement SQL script.

        This method splits the script into individual statements and executes
        them sequentially. Useful for applying schema files or migration scripts.

        Args:
            script: SQL script containing one or more statements separated by semicolons

        Raises:
            RuntimeError: If database is not connected
            Exception: If any statement fails to execute
        """
        if not self._initialized:
            raise RuntimeError("Database not connected")

        # Split script into individual statements
        # Simple split on semicolon - for complex SQL, use initialize_schema()
        statements = [s.strip() for s in script.split(';') if s.strip()]

        async with self.transaction() as session:
            for statement in statements:
                try:
                    await session.execute(text(statement))
                except Exception as e:
                    self._logger.error(
                        "execute_script_statement_failed",
                        statement=statement[:100],
                        error=str(e)
                    )
                    raise

        self._logger.debug("execute_script_completed", statements=len(statements))

    async def insert(self, table: str, data: Dict[str, Any]) -> Any:
        """Insert a row and return the first column's value (mirrors SQLite client)."""
        if not self._initialized:
            raise RuntimeError("Database not connected")

        # Import uuid_str for UUID handling at ingestion layer
        from jeeves_shared.uuid_utils import uuid_str

        # Column constants imported from jeeves_avionics.database.constants
        # Convert values for special column types
        converted_data = {}
        placeholder_parts = []

        for key, value in data.items():
            if key in UUID_COLUMNS and value is not None:
                converted_data[key] = uuid_str(value)
                placeholder_parts.append(f":{key}")
            elif key in VECTOR_COLUMNS and value is not None:
                # Convert list/numpy array to string for pgvector
                if hasattr(value, 'tolist'):
                    value = value.tolist()
                converted_data[key] = str(value)
                # Use CAST() syntax instead of :: to avoid SQLAlchemy parameter parsing issues
                placeholder_parts.append(f"CAST(:{key} AS vector)")
            elif key in JSONB_COLUMNS and value is not None:
                # Ensure JSONB columns are JSON strings
                if isinstance(value, (dict, list)):
                    converted_data[key] = json.dumps(value, cls=JSONEncoderWithUUID)
                else:
                    converted_data[key] = value
                placeholder_parts.append(f":{key}")
            else:
                converted_data[key] = value
                placeholder_parts.append(f":{key}")

        columns = ", ".join(converted_data.keys())
        placeholders = ", ".join(placeholder_parts)
        query = f"INSERT INTO {table} ({columns}) VALUES ({placeholders})"

        async with self.session() as session:
            await session.execute(text(query), converted_data)
            await session.commit()

        first_key = next(iter(data))
        return data[first_key]

    async def update(
        self,
        table: str,
        data: Dict[str, Any],
        where_clause: str,
        where_params: Optional[Any] = None
    ) -> int:
        """Update rows and return the number of affected rows."""
        if not self._initialized:
            raise RuntimeError("Database not connected")

        # Import uuid_str for UUID handling at ingestion layer
        from jeeves_shared.uuid_utils import uuid_str

        # Column constants imported from jeeves_avionics.database.constants
        # Convert UUID values for known UUID columns
        converted_data = {}
        for key, value in data.items():
            if key in UUID_COLUMNS and value is not None:
                converted_data[key] = uuid_str(value)
            else:
                converted_data[key] = value

        set_clause = ", ".join([f"{key} = :{key}" for key in converted_data.keys()])
        base_query = f"UPDATE {table} SET {set_clause} WHERE {where_clause}"
        query, prepared_where_params = self._prepare_query_and_params(base_query, where_params)

        params = {**converted_data, **prepared_where_params}

        async with self.session() as session:
            result = await session.execute(text(query), params)
            await session.commit()
            return result.rowcount

    async def fetch_one(
        self,
        query: str,
        parameters: Optional[Dict[str, Any]] = None,
        commit: bool = False
    ) -> Optional[Dict[str, Any]]:
        """Fetch one row from query results.

        Args:
            query: SQL query string
            parameters: Dictionary of parameters
            commit: Whether to commit the transaction (use True for INSERT...RETURNING)

        Returns:
            Dictionary representing the row, or None if no results
        """
        if not self._initialized:
            raise RuntimeError("Database not connected")

        prepared_query, prepared_params = self._prepare_query_and_params(query, parameters)

        async with self.session() as session:
            result = await session.execute(text(prepared_query), prepared_params)
            row = result.fetchone()
            if commit:
                await session.commit()
            if row is None:
                return None

            # Convert Row to dict
            return dict(row._mapping)

    async def fetch_all(
        self,
        query: str,
        parameters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """Fetch all rows from query results.

        Args:
            query: SQL query string
            parameters: Dictionary of parameters

        Returns:
            List of dictionaries representing rows
        """
        if not self._initialized:
            raise RuntimeError("Database not connected")

        prepared_query, prepared_params = self._prepare_query_and_params(query, parameters)

        async with self.session() as session:
            result = await session.execute(text(prepared_query), prepared_params)
            rows = result.fetchall()

            # Convert Rows to dicts
            return [dict(row._mapping) for row in rows]

    async def health_check(self) -> Dict[str, Any]:
        """Perform a health check on the database connection.

        Returns:
            Dictionary with health check results
        """
        try:
            if not self._initialized:
                return {
                    "status": "unhealthy",
                    "error": "Database not connected"
                }

            # Basic connection test
            async with self.session() as session:
                result = await session.execute(text("SELECT 1"))
                result.scalar()

            # Get pool stats
            if self.engine:
                pool = self.engine.pool
                pool_stats = {
                    "size": pool.size(),
                    "checked_in": pool.checkedin(),
                    "checked_out": pool.checkedout(),
                    "overflow": pool.overflow(),
                }
            else:
                pool_stats = {}

            return {
                "status": "healthy",
                "backend": "postgresql",
                "pool": pool_stats,
            }

        except Exception as e:
            self._logger.error("health_check_failed", error=str(e))
            return {
                "status": "unhealthy",
                "error": str(e)
            }

    async def get_table_stats(self, table_name: str) -> Dict[str, Any]:
        """Get statistics for a specific table.

        Args:
            table_name: Name of the table

        Returns:
            Dictionary with table statistics
        """
        query = """
        SELECT
            schemaname,
            tablename,
            pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS total_size,
            pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) AS table_size,
            pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) -
                          pg_relation_size(schemaname||'.'||tablename)) AS indexes_size,
            n_live_tup AS row_count
        FROM pg_tables
        LEFT JOIN pg_stat_user_tables ON pg_tables.tablename = pg_stat_user_tables.relname
        WHERE schemaname = 'public' AND tablename = :table_name
        """

        result = await self.fetch_one(query, {"table_name": table_name})
        return result or {}

    async def get_all_tables_stats(self) -> List[Dict[str, Any]]:
        """Get statistics for all tables.

        Returns:
            List of dictionaries with table statistics
        """
        query = """
        SELECT
            tablename,
            pg_size_pretty(pg_total_relation_size('public.'||tablename)) AS total_size,
            pg_size_pretty(pg_relation_size('public.'||tablename)) AS table_size,
            n_live_tup AS row_count
        FROM pg_tables
        LEFT JOIN pg_stat_user_tables ON pg_tables.tablename = pg_stat_user_tables.relname
        WHERE schemaname = 'public'
        ORDER BY pg_total_relation_size('public.'||tablename) DESC
        """

        return await self.fetch_all(query)

    async def vacuum_analyze(self, table_name: Optional[str] = None):
        """Run VACUUM ANALYZE on a table or all tables.

        Args:
            table_name: Name of the table, or None for all tables
        """
        if table_name:
            query = f"VACUUM ANALYZE {table_name}"
        else:
            query = "VACUUM ANALYZE"

        # VACUUM cannot run inside a transaction block
        # Use raw connection with autocommit
        # Note: AsyncConnection.execution_options() is async in SQLAlchemy 2.0
        async with self.engine.connect() as conn:
            conn_with_autocommit = await conn.execution_options(isolation_level="AUTOCOMMIT")
            await conn_with_autocommit.execute(text(query))

        self._logger.info("vacuum_analyze_completed", table=table_name or "all")

    # Note: Use to_json() and from_json() from jeeves_shared.serialization
    # for JSON serialization (imported at module level)
